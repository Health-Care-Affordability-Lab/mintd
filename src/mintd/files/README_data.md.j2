# {{ full_project_name }}

Data product for {{ project_name }}.

## Overview

This repository contains the data processing pipeline for {{ project_name }} using **{{ language|title }}** as the primary programming language.

## Project Goal

**TODO**: Describe the specific goal and purpose of this data repository. What research question does it address? What data sources are being processed?

## Data Availability and Provenance Statements

### Statement about Rights

- [ ] I certify that the author(s) of the manuscript have legitimate access to and permission to use the data used in this manuscript.

### License for Data

The data are licensed under a [Creative Commons Attribution 4.0 International License (CC-BY 4.0)](https://creativecommons.org/licenses/by/4.0/).

**TODO**: Update this section with the appropriate license for your data.

### Data Sources

| Data Source | Location | Provided | Citation |
|-------------|----------|----------|----------|
| [Source 1] | [URL or description] | Yes/No | See `citations.md` |

**TODO**: Document all data sources used in this project.

## Data Flow

The data processing follows a standard two-stage pipeline:

- **Raw Data** → `data/raw/`: Original, unmodified data as acquired from sources
- **Analysis Data** → `data/analysis/`: Processed and cleaned data ready for analysis

## Requirements

{% if language == "python" %}
Install Python dependencies:
```bash
pip install -r requirements.txt
```
{% elif language == "r" %}
Install R dependencies:
```r
# Install renv for reproducible environments
install.packages("renv")
renv::restore()  # Install dependencies from renv.lock
```
{% elif language == "stata" %}
This project requires Stata for data processing. Any Python dependencies for auxiliary scripts:
```bash
pip install -r requirements.txt
```
{% endif %}

## Computational Requirements

### Controlled Randomness

- Random seed: Set in `code/config.{{ 'R' if language == 'r' else ('do' if language == 'stata' else 'py') }}` (if applicable)

### Memory, Runtime, and Storage Requirements

**TODO**: Document the computational requirements:

- Approximate runtime: [X hours/minutes]
- Memory requirements: [X GB RAM]
- Storage requirements: [X GB disk space]

## Pipeline Scripts

{% if language == "python" %}
1. **Ingest**: `code/ingest.py` - Raw data acquisition and initial processing
2. **Clean**: `code/clean.py` - Data cleaning and preprocessing
3. **Validate**: `code/validate.py` - Data quality checks and validation
{% elif language == "r" %}
1. **Ingest**: `code/ingest.R` - Raw data acquisition and initial processing
2. **Clean**: `code/clean.R` - Data cleaning and preprocessing
3. **Validate**: `code/validate.R` - Data quality checks and validation
{% elif language == "stata" %}
1. **Ingest**: `code/ingest.do` - Raw data acquisition and initial processing
2. **Clean**: `code/clean.do` - Data cleaning and preprocessing
3. **Validate**: `code/validate.do` - Data quality checks and validation
{% endif %}

## Instructions to Replicators

{% if language == "python" %}
Run the complete pipeline:
```bash
# Ingest raw data
python code/ingest.py

# Clean data
python code/clean.py

# Validate results
python code/validate.py
```
{% elif language == "r" %}
Run the complete pipeline:
```r
# Ingest raw data
source("code/ingest.R")

# Clean data
source("code/clean.R")

# Validate results
source("code/validate.R")
```

Or from command line:
```bash
Rscript code/ingest.R
Rscript code/clean.R
Rscript code/validate.R
```
{% elif language == "stata" %}
Run the complete pipeline in Stata:
```stata
// Ingest raw data
do code/ingest.do

// Clean data
do code/clean.do

// Validate results
do code/validate.do
```
{% endif %}

## Extending the Pipeline

For more complex data ingestion scenarios, consider:

- **Multiple Data Sources**: Modify the ingest script to handle multiple APIs/databases
- **Incremental Updates**: Add logic to check for new data since last run
- **Data Versioning**: Use DVC to track changes in raw data files
- **Parallel Processing**: Split large datasets across multiple workers
- **Error Handling**: Add robust error handling and logging
- **Configuration Files**: Move hardcoded paths and parameters to config files

## Data Validation

The validation script performs checks for:
- Missing values and data completeness
- Duplicate records
- Data type consistency
- Basic statistical distributions
- Business rule compliance

**TODO**: Add specific validation rules relevant to your data domain.

## References

See `citations.md` for data citations and references.

## Data Provenance

Created: {{ created_at }}
{% if author %}Author: {{ author }}{% endif %}
{% if organization %}Organization: {{ organization }}{% endif %}

---
This project was created by and is managed by [mintd](https://github.com/cooper-lab/mint).
