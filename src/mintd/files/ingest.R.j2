#' Data ingestion for {{ project_name }}
#'
#' This script handles downloading and initial processing of raw data.
#' NOTE: This script runs from the code/ directory. Data paths use ../data/

# =============================================================================
# UPDATING FOR NEW DATA (e.g., annual releases)
# =============================================================================
# Data versioning is handled by DVC. When new data becomes available:
#
# 1. Update this script to point to the new data source
# 2. Run the pipeline: dvc repro
# 3. Commit changes: git add . && git commit -m "Update to 2024 data"
# 4. Push data and code: dvc push && git push
#
# DVC tracks all versions - use `dvc diff` to see changes between versions
# and `git log` / `dvc checkout` to access previous versions.
# =============================================================================

# Import mint utilities (located in same code/ directory)
source("_mintd_utils.R")

#' List and log all files in the raw data directory
list_raw_files <- function(logger) {
  raw_files <- list.files(RAW_DIR, full.names = TRUE)

  if (length(raw_files) > 0) {
    logger$log("Found ", length(raw_files), " files in RAW_DIR:")
    for (f in raw_files) {
      size_kb <- file.info(f)$size / 1024
      logger$log("  - ", basename(f), " (", round(size_kb, 1), " KB)")
    }
  } else {
    logger$log("No files found in RAW_DIR. Add raw data files to get started.")
  }

  return(raw_files)
}

main <- function() {
  # Initialize logging (validates directory and creates log file)
  logger <- ParameterAwareLogger("ingest")
  logger$log("Starting data ingestion for {{ project_name }}...")

  # Ensure data directories exist (using global paths from _mintd_utils.R)
  paths <- get_data_paths()
  lapply(paths, function(d) {
    if (!dir.exists(d)) dir.create(d, recursive = TRUE)
  })

  logger$log("Directories ready:")
  logger$log("  Raw data: ", RAW_DIR)
  logger$log("  Intermediate: ", INTERMEDIATE_DIR)
  logger$log("  Final data: ", FINAL_DIR)

  # List existing raw files
  raw_files <- list_raw_files(logger)

  # ==========================================================================
  # CUSTOMIZE: Add your data ingestion logic below
  # ==========================================================================
  # Examples of common ingestion patterns:
  #
  # 1. Download from URL:
  #    library(httr)
  #    response <- GET("https://example.com/data.csv")
  #    writeBin(content(response, "raw"), file.path(RAW_DIR, "data.csv"))
  #
  # 2. Copy from local source:
  #    file.copy("/path/to/source/data.csv", file.path(RAW_DIR, "data.csv"))
  #
  # 3. Read from database:
  #    library(DBI)
  #    con <- dbConnect(...)
  #    df <- dbGetQuery(con, "SELECT * FROM table")
  #    readr::write_csv(df, file.path(RAW_DIR, "data.csv"))
  # ==========================================================================

  logger$log("Data ingestion completed successfully.")
  logger$close()
}

if (!interactive()) {
  main()
}
