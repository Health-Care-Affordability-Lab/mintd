"""mintd utility functions for {{ project_name }}.

This module provides common utilities used across all project scripts.
DO NOT MODIFY - This file is managed by mintd and will be overwritten.

NOTE: Scripts are expected to run from the code/ directory.
Data paths should use "../data/" or the get_data_paths() function.
"""

import json
from pathlib import Path
from typing import Dict, Any, Optional, List


# =============================================================================
# PATH DEFINITIONS (relative to code/ where scripts run from)
# =============================================================================
DATA_DIR = Path("../data")
RAW_DIR = Path("../data/raw")
ANALYSIS_DIR = Path("../data/analysis")


def get_data_paths() -> Dict[str, Path]:
    """Get standard data directory paths.

    Returns:
        Dict with keys: 'data', 'raw', 'analysis'
    """
    return {
        'data': DATA_DIR,
        'raw': RAW_DIR,
        'analysis': ANALYSIS_DIR,
    }


def setup_project_directory() -> Path:
    """Validate we're running from code/ directory and return project root.

    This function checks if we're in the correct directory structure:
    - Preferred: Running from code/ (project indicators in parent)
    - Fallback: Running from project root (for backwards compatibility)

    Returns:
        Path: Project root directory

    Raises:
        RuntimeError: If not running from expected directory
    """
    global DATA_DIR, RAW_DIR, ANALYSIS_DIR

    current_dir = Path.cwd()
    # Look for project root indicators
    root_indicators = ['metadata.json', '.git']
    # Check if we're in code/ (project indicators in parent directory)
    parent_dir = current_dir.parent
    if any((parent_dir / indicator).exists() for indicator in root_indicators):
        # We're in code/, paths are already correct
        return parent_dir
    # Check if we're in project root (backwards compatibility)
    if any((current_dir / indicator).exists() for indicator in root_indicators):
        # Update paths for project-root execution
        DATA_DIR = Path("data")
        RAW_DIR = Path("data/raw")
        ANALYSIS_DIR = Path("data/analysis")
        return current_dir
    # Not in a valid location
    raise RuntimeError(
        f"Not running from expected directory.\n"
        f"Scripts should be run from code/ directory.\n"
        f"Current directory: {current_dir}\n"
        f"Please cd to the project's code/ directory and try again."
    )


def generate_data_schema(data_path: Path, output_path: Optional[Path] = None) -> Dict[str, Any]:
    """Generate JSON schema from data file.

    Args:
        data_path: Path to data file (CSV, JSON, etc.)
        output_path: Optional path to save schema JSON

    Returns:
        Dict containing schema information
    """
    import pandas as pd

    try:
        # Read data based on file extension
        if data_path.suffix.lower() == '.csv':
            df = pd.read_csv(data_path)
        elif data_path.suffix.lower() == '.json':
            df = pd.read_json(data_path)
        elif data_path.suffix.lower() in ['.xlsx', '.xls']:
            df = pd.read_excel(data_path)
        else:
            raise ValueError(f"Unsupported file format: {data_path.suffix}")

        # Generate schema
        schema = {
            "filename": data_path.name,
            "filepath": str(data_path),
            "observations": len(df),
            "columns": len(df.columns),
            "variables": []
        }

        for col in df.columns:
            # Basic type detection
            dtype = str(df[col].dtype)
            if dtype.startswith('int'):
                var_type = "integer"
            elif dtype.startswith('float'):
                var_type = "numeric"
            elif dtype == 'object':
                var_type = "string"
            elif dtype.startswith('bool'):
                var_type = "boolean"
            else:
                var_type = dtype

            # Generate basic label from column name
            label = col.replace('_', ' ').replace('-', ' ').title()

            variable_info = {
                "name": col,
                "type": var_type,
                "label": label,
                "missing_count": int(df[col].isna().sum()),
                "unique_values": int(df[col].nunique()) if df[col].dtype == 'object' else None
            }

            # Add numeric statistics for numeric columns
            if var_type in ["integer", "numeric"]:
                variable_info.update({
                    "min": float(df[col].min()) if not pd.isna(df[col].min()) else None,
                    "max": float(df[col].max()) if not pd.isna(df[col].max()) else None,
                    "mean": float(df[col].mean()) if not pd.isna(df[col].mean()) else None,
                    "std": float(df[col].std()) if not pd.isna(df[col].std()) else None
                })

            schema["variables"].append(variable_info)

        # Save schema if output path provided
        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w') as f:
                json.dump(schema, f, indent=2, default=str)

        return schema

    except Exception as e:
        raise RuntimeError(f"Failed to generate schema for {data_path}: {e}")


def find_data_files(directory: Path, extensions: List[str] = None) -> List[Path]:
    """Find data files in a directory.

    Args:
        directory: Directory to search
        extensions: File extensions to look for (default: common data formats)

    Returns:
        List of data file paths
    """
    if extensions is None:
        extensions = ['.csv', '.json', '.xlsx', '.xls', '.dta', '.rds', '.sav']

    data_files = []
    for ext in extensions:
        data_files.extend(directory.glob(f"**/*{ext}"))

    return sorted(data_files)
