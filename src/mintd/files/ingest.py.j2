"""Data ingestion script for {{ project_name }}.

NOTE: This script runs from the code/ directory. Data paths use ../data/
"""

import sys
from pathlib import Path

# Import mint utilities (located in same code/ directory)
from _mintd_utils import (
    setup_project_directory,
    ParameterAwareLogger,
    RAW_DIR,
    INTERMEDIATE_DIR,
    FINAL_DIR,
    get_data_paths
)

# =============================================================================
# UPDATING FOR NEW DATA (e.g., annual releases)
# =============================================================================
# Data versioning is handled by DVC. When new data becomes available:
#
# 1. Update this script to point to the new data source
# 2. Run the pipeline: dvc repro
# 3. Commit changes: git add . && git commit -m "Update to 2024 data"
# 4. Push data and code: dvc push && git push
#
# DVC tracks all versions - use `dvc diff` to see changes between versions
# and `git log` / `dvc checkout` to access previous versions.
# =============================================================================


def list_raw_files(logger):
    """List and log all files in the raw data directory."""
    raw_files = list(RAW_DIR.glob("*"))
    if raw_files:
        logger.log(f"Found {len(raw_files)} files in RAW_DIR:")
        for f in raw_files:
            size_kb = f.stat().st_size / 1024 if f.is_file() else 0
            logger.log(f"  - {f.name} ({size_kb:.1f} KB)")
    else:
        logger.log("No files found in RAW_DIR. Add raw data files to get started.")
    return raw_files


def main():
    """Main ingestion workflow."""
    # Initialize logging (validates directory and creates log file)
    logger = ParameterAwareLogger("ingest")
    logger.log("Starting data ingestion for {{ project_name }}...")

    # Get data paths and ensure directories exist
    paths = get_data_paths()
    for name, path in paths.items():
        if name != 'logs':  # logs already created by logger
            path.mkdir(parents=True, exist_ok=True)

    logger.log(f"Raw data directory: {RAW_DIR}")
    logger.log(f"Intermediate data directory: {INTERMEDIATE_DIR}")
    logger.log(f"Final data directory: {FINAL_DIR}")

    # List existing raw files
    raw_files = list_raw_files(logger)

    # ==========================================================================
    # CUSTOMIZE: Add your data ingestion logic below
    # ==========================================================================
    # Examples of common ingestion patterns:
    #
    # 1. Download from URL:
    #    import requests
    #    response = requests.get("https://example.com/data.csv")
    #    with open(RAW_DIR / "data.csv", 'wb') as f:
    #        f.write(response.content)
    #
    # 2. Copy from local source:
    #    import shutil
    #    shutil.copy("/path/to/source/data.csv", RAW_DIR / "data.csv")
    #
    # 3. Query database:
    #    import pandas as pd
    #    df = pd.read_sql("SELECT * FROM table", connection)
    #    df.to_csv(RAW_DIR / "data.csv", index=False)
    # ==========================================================================

    logger.log("Data ingestion completed successfully.")
    logger.close()


if __name__ == "__main__":
    main()
