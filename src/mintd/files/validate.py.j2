"""Data validation script for {{ project_name }}.

NOTE: This script runs from the code/ directory. Data paths use ../data/
"""

import sys
import json
from pathlib import Path
from datetime import datetime
import pandas as pd

# Import mint utilities (located in same code/ directory)
from _mintd_utils import (
    setup_project_directory,
    ParameterAwareLogger,
    RAW_DIR,
    INTERMEDIATE_DIR,
    FINAL_DIR,
    get_data_paths
)


def validate_dataset(df, dataset_name, logger):
    """Run validation checks on a dataset.

    Args:
        df: pandas DataFrame to validate
        dataset_name: Name of the dataset for logging
        logger: Logger instance

    Returns:
        dict with validation results
    """
    results = {
        'dataset': dataset_name,
        'rows': len(df),
        'columns': len(df.columns),
        'issues': [],
        'passed': True
    }

    # Check for missing values
    missing_cols = df.columns[df.isnull().any()].tolist()
    if missing_cols:
        missing_counts = {col: int(df[col].isnull().sum()) for col in missing_cols}
        results['issues'].append({
            'type': 'missing_values',
            'columns': missing_counts
        })

    # Check for duplicate rows
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        results['issues'].append({
            'type': 'duplicate_rows',
            'count': int(duplicates)
        })

    # Check data types
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    results['numeric_columns'] = len(numeric_cols)

    # Log validation results
    logger.log(f"Validation for {dataset_name}:")
    logger.log(f"  Rows: {results['rows']}")
    logger.log(f"  Columns: {results['columns']}")
    logger.log(f"  Numeric columns: {results['numeric_columns']}")

    if results['issues']:
        results['passed'] = False
        logger.log("  Issues found:")
        for issue in results['issues']:
            if issue['type'] == 'missing_values':
                logger.log(f"    - Missing values in: {list(issue['columns'].keys())}")
            elif issue['type'] == 'duplicate_rows':
                logger.log(f"    - Found {issue['count']} duplicate rows")
    else:
        logger.log("  [OK] No issues found")

    return results


def main():
    """Main validation workflow."""
    # Initialize logging (validates directory and creates log file)
    logger = ParameterAwareLogger("validate")
    logger.log("Starting data validation for {{ project_name }}...")

    # Get data paths and ensure directories exist
    paths = get_data_paths()
    for name, path in paths.items():
        if name != 'logs':  # logs already created by logger
            path.mkdir(parents=True, exist_ok=True)

    logger.log(f"Raw data directory: {RAW_DIR}")
    logger.log(f"Intermediate data directory: {INTERMEDIATE_DIR}")
    logger.log(f"Final data directory: {FINAL_DIR}")

    # Validate all CSV files in INTERMEDIATE_DIR
    intermediate_files = list(INTERMEDIATE_DIR.glob("*.csv"))

    if not intermediate_files:
        logger.log("No CSV files found in INTERMEDIATE_DIR. Run clean.py first.")
        validation_report = {'status': 'no_files', 'datasets': []}
    else:
        logger.log(f"Found {len(intermediate_files)} CSV files to validate")

        all_results = []
        all_valid = True

        for data_file in intermediate_files:
            df = pd.read_csv(data_file)
            results = validate_dataset(df, data_file.stem, logger)
            all_results.append(results)
            all_valid = all_valid and results['passed']

        # Summary
        if all_valid:
            logger.log("[OK] All datasets passed validation")
        else:
            logger.log("[WARN] Some datasets have issues - review above")

        # Create validation report
        validation_report = {
            'status': 'passed' if all_valid else 'issues_found',
            'timestamp': datetime.now().isoformat(),
            'datasets': all_results
        }

    # Save validation report to FINAL_DIR
    report_file = FINAL_DIR / "validation_report.json"
    with open(report_file, 'w') as f:
        json.dump(validation_report, f, indent=2)
    logger.log(f"Validation report saved to: {report_file}")

    logger.log("Data validation completed successfully.")
    logger.close()


if __name__ == "__main__":
    main()
