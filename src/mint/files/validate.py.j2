"""Data validation script for {{ project_name }}."""

import sys
from pathlib import Path
import pandas as pd

# Import mint utilities
from _mint_utils import setup_project_directory, ParameterAwareLogger

def set_directories():
    """Set up standard directory structure."""
    project_root = setup_project_directory()

    dirs = {
        'raw': project_root / 'data' / 'raw',
        'intermediate': project_root / 'data' / 'intermediate',
        'final': project_root / 'data' / 'final'
    }

    for dir_path in dirs.values():
        dir_path.mkdir(parents=True, exist_ok=True)

    return dirs

def validate_dataset(df, dataset_name, logger):
    """Run validation checks on a dataset."""
    issues = []

    # Check for missing values
    missing_cols = df.columns[df.isnull().any()].tolist()
    if missing_cols:
        issues.append(f"Missing values in columns: {missing_cols}")

    # Check for duplicate rows
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        issues.append(f"Found {duplicates} duplicate rows")

    # Check data types
    numeric_cols = df.select_dtypes(include=['number']).columns
    if len(numeric_cols) == 0:
        issues.append("No numeric columns found")

    # Basic statistics
    logger.log(f"Validation for {dataset_name}:")
    logger.log(f"  Rows: {len(df)}")
    logger.log(f"  Columns: {len(df.columns)}")
    logger.log(f"  Numeric columns: {len(numeric_cols)}")

    if issues:
        logger.log("  Issues found:")
        for issue in issues:
            logger.log(f"    - {issue}")
    else:
        logger.log("  ✓ No issues found")

    return len(issues) == 0

def main():
    """Main validation workflow."""
    # Initialize logging (filename will include any parameters passed)
    logger = ParameterAwareLogger("validate")
    logger.log("Starting data validation for {{ project_name }}...")

    dirs = set_directories()

    logger.log(f"Raw data directory: {dirs['raw']}")
    logger.log(f"Intermediate data directory: {dirs['intermediate']}")
    logger.log(f"Final data directory: {dirs['final']}")

    # TODO: Implement comprehensive validation logic
    # - Load cleaned data from data/intermediate/
    # - Run quality checks (missing values, duplicates, data types)
    # - Business rule validation
    # - Statistical checks
    # - Save validation reports

    # Example validation
    # intermediate_files = list(dirs['intermediate'].glob("*.csv"))
    # all_valid = True
    #
    # for intermediate_file in intermediate_files:
    #     df = pd.read_csv(intermediate_file)
    #     is_valid = validate_dataset(df, intermediate_file.stem, logger)
    #     all_valid = all_valid and is_valid
    #
    # if all_valid:
    #     logger.log("✓ All datasets passed validation")
    # else:
    #     logger.log("✗ Some datasets failed validation - review issues above")

    logger.log("Data validation completed successfully.")
    logger.close()

if __name__ == "__main__":
    main()