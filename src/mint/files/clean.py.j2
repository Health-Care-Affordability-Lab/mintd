"""Data cleaning script for {{ project_name }}."""

import sys
from pathlib import Path
import pandas as pd

# Import mint utilities
from _mint_utils import setup_project_directory, ParameterAwareLogger

def set_directories():
    """Set up standard directory structure."""
    project_root = setup_project_directory()

    dirs = {
        'raw': project_root / 'data' / 'raw',
        'intermediate': project_root / 'data' / 'intermediate',
        'final': project_root / 'data' / 'final'
    }

    for dir_path in dirs.values():
        dir_path.mkdir(parents=True, exist_ok=True)

    return dirs

def main():
    """Main cleaning workflow."""
    # Initialize logging (filename will include any parameters passed)
    logger = ParameterAwareLogger("clean")
    logger.log("Starting data cleaning for {{ project_name }}...")

    dirs = set_directories()

    logger.log(f"Raw data directory: {dirs['raw']}")
    logger.log(f"Intermediate data directory: {dirs['intermediate']}")
    logger.log(f"Final data directory: {dirs['final']}")

    # TODO: Implement data cleaning logic
    # - Load raw data from data/raw/
    # - Handle missing values, data types, outliers
    # - Standardize formats and encodings
    # - Save cleaned data to data/intermediate/

    # Example:
    # # Load raw data
    # raw_files = list(dirs['raw'].glob("*.csv"))
    # for raw_file in raw_files:
    #     df = pd.read_csv(raw_file)
    #
    #     # Basic cleaning
    #     df = df.dropna()  # Remove missing values
    #     df.columns = df.columns.str.lower().str.replace(' ', '_')
    #
    #     # Save cleaned data
    #     intermediate_file = dirs['intermediate'] / f"clean_{raw_file.name}"
    #     df.to_csv(intermediate_file, index=False)
    #     logger.log(f"Cleaned {len(df)} rows, saved to {intermediate_file}")

    logger.log("Data cleaning completed successfully.")
    logger.close()

if __name__ == "__main__":
    main()