"""Data ingestion script for {{ project_name }}."""

import sys
from pathlib import Path

# Import mint utilities
from _mint_utils import setup_project_directory, ParameterAwareLogger

# =============================================================================
# UPDATING FOR NEW DATA (e.g., annual releases)
# =============================================================================
# Data versioning is handled by DVC. When new data becomes available:
#
# 1. Update this script to point to the new data source
# 2. Run the pipeline: dvc repro
# 3. Commit changes: git add . && git commit -m "Update to 2024 data"
# 4. Push data and code: dvc push && git push
#
# DVC tracks all versions - use `dvc diff` to see changes between versions
# and `git log` / `dvc checkout` to access previous versions.
# =============================================================================

def set_directories():
    """Set up standard directory structure."""
    project_root = setup_project_directory()

    dirs = {
        'raw': project_root / 'data' / 'raw',
        'intermediate': project_root / 'data' / 'intermediate',
        'final': project_root / 'data' / 'final'
    }

    for dir_path in dirs.values():
        dir_path.mkdir(parents=True, exist_ok=True)

    return dirs

def main():
    """Main ingestion workflow."""
    # Initialize logging (filename will include any parameters passed)
    logger = ParameterAwareLogger("ingest")
    logger.log("Starting data ingestion for {{ project_name }}...")

    dirs = set_directories()

    logger.log(f"Raw data directory: {dirs['raw']}")
    logger.log(f"Intermediate data directory: {dirs['intermediate']}")
    logger.log(f"Final data directory: {dirs['final']}")

    # TODO: Implement data ingestion logic
    # - Download data from APIs/databases
    # - Extract archives using zipfile/patool
    # - Initial data validation with pandas
    # - Save to data/raw/

    # Example:
    # import requests
    # import pandas as pd
    #
    # # Download data
    # response = requests.get("https://example.com/data.csv")
    # raw_file = dirs['raw'] / "downloaded_data.csv"
    # with open(raw_file, 'wb') as f:
    #     f.write(response.content)
    #
    # # Quick validation
    # df = pd.read_csv(raw_file)
    # logger.log(f"Downloaded {len(df)} rows with columns: {list(df.columns)}")

    logger.log("Data ingestion completed successfully.")
    logger.close()

if __name__ == "__main__":
    main()